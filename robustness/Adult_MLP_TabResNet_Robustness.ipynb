{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1747891080103,
     "user": {
      "displayName": "Hung Thanh Do",
      "userId": "14602254202234053172"
     },
     "user_tz": -120
    },
    "id": "BCgA_YXFcCzx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hungd\\anaconda3\\envs\\fair-ai-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Optional\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from aif360.sklearn.datasets import fetch_german\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Data Loading and Preparation (label-encoded)\n",
    "# ---------------------------\n",
    "X, y = shap.datasets.adult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding (already done by SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting anf formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Split into training and test sets (we use test for evaluation and explanations)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=random_state, stratify=y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20838, 12)\n",
      "X_test shape: (6513, 12)\n",
      "y_train shape: (20838,)\n",
      "y_test shape: (6513,)\n",
      "X_val shape: (5210, 12)\n",
      "y_val shape: (5210,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X_train_np = X_train.values.astype(np.float32)\n",
    "X_val_np = X_val.values.astype(np.float32)\n",
    "X_test_np = X_test.values.astype(np.float32)\n",
    "\n",
    "# For CrossEntropyLoss, labels must be integers.\n",
    "y_train_np = y_train.astype(np.int64).values if isinstance(y_train, pd.Series) else y_train.astype(np.int64)\n",
    "y_val_np = y_val.astype(np.int64).values if isinstance(y_val, pd.Series) else y_val.astype(np.int64)\n",
    "y_test_np = y_test.astype(np.int64).values if isinstance(y_test, pd.Series) else y_test.astype(np.int64)\n",
    "\n",
    "# Create PyTorch tensors\n",
    "X_train_t = torch.from_numpy(X_train_np)\n",
    "y_train_t = torch.from_numpy(y_train_np)\n",
    "X_val_t = torch.from_numpy(X_val_np)\n",
    "y_val_t = torch.from_numpy(y_val_np)\n",
    "X_test_t = torch.from_numpy(X_test_np)\n",
    "y_test_t = torch.from_numpy(y_test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM     = X_test_np.shape[1]\n",
    "NUM_CLASSES   = 2\n",
    "HIDDEN_DIM    = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "torch.manual_seed(random_state)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        # Initialize weights uniformly in [-1/√fan_in, +1/√fan_in]\n",
    "        nn.init.uniform_(self.fc1.weight,\n",
    "                         a=-1.0 / (input_dim**0.5),\n",
    "                         b=+1.0 / (input_dim**0.5))\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.uniform_(self.fc2.weight,\n",
    "                         a=-1.0 / (hidden_dim**0.5),\n",
    "                         b=+1.0 / (hidden_dim**0.5))\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # raw logits\n",
    "\n",
    "# Instantiate and move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlp_model = MLP(INPUT_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hungd\\AppData\\Local\\Temp\\ipykernel_15580\\2345738713.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mlp_model.load_state_dict(torch.load(\"saved_model/mlp_adults.pth\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "mlp_model.load_state_dict(torch.load(\"saved_model/mlp_adults.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.8248\n"
     ]
    }
   ],
   "source": [
    "X_test_t = X_test_t.to(device)\n",
    "y_test_t = y_test_t.to(device)\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = mlp_model(X_test_t)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    accuracy = (preds == y_test_t).float().mean().item()\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Model Definition: TabResNet (using residual blocks)\n",
    "# ---------------------------\n",
    "class TabResBlock(nn.Module):\n",
    "    def __init__(self, d_block, d_hidden, dropout1, dropout2):\n",
    "        super(TabResBlock, self).__init__()\n",
    "        # Sub-block 1\n",
    "        self.bn1 = nn.BatchNorm1d(d_block)\n",
    "        self.ln1 = nn.Linear(d_block, d_hidden)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(dropout1)\n",
    "        # Sub-block 2\n",
    "        self.ln2 = nn.Linear(d_hidden, d_block)\n",
    "        self.drop2 = nn.Dropout(dropout2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.drop2(out)\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "class TabResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: Optional[int],\n",
    "        n_blocks: int,\n",
    "        d_block: int,\n",
    "        d_hidden: Optional[int],\n",
    "        d_hidden_multiplier: Optional[float] = 2,\n",
    "        dropout1: float = 0.2,\n",
    "        dropout2: float = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_in: Number of input features.\n",
    "            d_out: Number of outputs (set to 2 for binary classification).\n",
    "            n_blocks: Number of residual blocks.\n",
    "            d_block: Block width (input and output dimension of each block).\n",
    "            d_hidden: Hidden dimension inside a block.\n",
    "        \"\"\"\n",
    "        super(TabResNet, self).__init__()\n",
    "        # Project input to block width\n",
    "        self.input_projection = nn.Linear(d_in, d_block)\n",
    "        # Residual blocks\n",
    "        self.resblocks = nn.ModuleList([\n",
    "            TabResBlock(d_block, d_hidden, dropout1, dropout2) for _ in range(n_blocks)\n",
    "        ])\n",
    "        # Prediction block: outputs logits for d_out classes.\n",
    "        self.predblock = (\n",
    "            nn.Sequential(\n",
    "                nn.BatchNorm1d(d_block),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_block, d_out)\n",
    "            ) if d_out is not None else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        for block in self.resblocks:\n",
    "            x = block(x)\n",
    "        if self.predblock is not None:\n",
    "            x = self.predblock(x)\n",
    "        return x  # returns logits\n",
    "\n",
    "\n",
    "# Instantiate model; input dimension equals the number of columns in X_test_np.\n",
    "d_in = X_test_np.shape[1]\n",
    "model = TabResNet(\n",
    "    d_in=d_in,\n",
    "    d_out=2,       # Two outputs for binary classification.\n",
    "    n_blocks=2,\n",
    "    d_block=16,\n",
    "    d_hidden=32,\n",
    "    dropout1=0.2,\n",
    "    dropout2=0.05\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hungd\\AppData\\Local\\Temp\\ipykernel_15580\\3354692301.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"saved_model/tabresnet_adults.pth\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"saved_model/tabresnet_adults.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.8474\n"
     ]
    }
   ],
   "source": [
    "X_test_t = X_test_t.to(device)\n",
    "y_test_t = y_test_t.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    accuracy = (preds == y_test_t).float().mean().item()\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.metrics import clever_u, loss_sensitivity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "clip_values = (X_test_np.min(), X_test_np.max())\n",
    "\n",
    "num_samples = X_test_np.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ART classifier for TabResNet\n",
    "mlp_art_classifier = PyTorchClassifier(\n",
    "    model=mlp_model,\n",
    "    loss=criterion,\n",
    "    input_shape=X_test_np.shape[1],\n",
    "    nb_classes=2,\n",
    "    optimizer=optimizer,\n",
    "    clip_values=clip_values,\n",
    "    device_type=device\n",
    ")\n",
    "\n",
    "# Train the ART classifier with TabResNet\n",
    "# art_classifier.fit(X_train_np, y_train_np, batch_size=64, nb_epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuraacy Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 0.8248\n",
      "Accuracy on adversarial test examples: 0.8191\n",
      "Accuracy difference: 0.0057\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp_art_classifier.predict(X_test_t)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == y_test_np) / len(y_test_np)\n",
    "print(f\"Accuracy on benign test examples: {accuracy:.4f}\")\n",
    "\n",
    "# Generate adversarial test examples\n",
    "attack = FastGradientMethod(estimator=mlp_art_classifier, eps=0.2)\n",
    "x_test_adv = attack.generate(x=X_test_np)\n",
    "\n",
    "# Evaluate the ART classifier on adversarial test examples\n",
    "\n",
    "predictions = mlp_art_classifier.predict(x_test_adv)\n",
    "accuracy_adv = np.sum(np.argmax(predictions, axis=1) == y_test_np) / len(y_test_np)\n",
    "print(f\"Accuracy on adversarial test examples: {accuracy_adv:.4f}\")\n",
    "\n",
    "print(f\"Accuracy difference: {accuracy - accuracy_adv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEVER-u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clever-u Score:\n",
    "\n",
    "score is a lower bound -> minimum perturbation size required to change the model's output to any wrong label\n",
    "\n",
    "“directional probes” (here 10*20=200) -> to get a reliable worst‐case slope estimate\n",
    "\n",
    "A higher score -> stronger local robustness\n",
    "\n",
    "range value = [0.0, radius]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in test set: 6513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing CLEVER-U: 100%|██████████| 6513/6513 [04:59<00:00, 21.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER-U score for TabResNet: 0.1956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clever_scores = []\n",
    "num_samples = X_test_np.shape[0]\n",
    "print(f\"Number of samples in test set: {num_samples}\")\n",
    "# for sample in X_test_np[:num_samples]: \n",
    "for sample in tqdm(X_test_np[:num_samples], desc=\"Computing CLEVER-U\"):\n",
    "    # sample is shape (D,) → np.array([sample]) → (1, D)\n",
    "    c = clever_u(\n",
    "        classifier=mlp_art_classifier,\n",
    "        x=sample,\n",
    "        nb_batches=20,\n",
    "        batch_size=10,             # you only have one sample in the “batch”\n",
    "        norm=2,\n",
    "        radius=0.2,\n",
    "        verbose=False\n",
    "    )\n",
    "    clever_scores.append(c)\n",
    "\n",
    "clever = float(np.mean(clever_scores))\n",
    "print(f\"CLEVER-U score for TabResNet: {clever:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Sensitivity\n",
    "\n",
    "A **higher** sensitivity -> small input perturbations can cause **larger** changes in the loss -> indicating a \"steeper\" or potentially **less** robust local region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss sensitivity over test set: 0.000025\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "# Convert integer labels to one-hot:\n",
    "y_test_onehot = np.eye(num_classes)[y_test_np]\n",
    "\n",
    "sensitivity = loss_sensitivity(\n",
    "    classifier=mlp_art_classifier,\n",
    "    x=X_test_np,\n",
    "    y=y_test_onehot\n",
    ")\n",
    "\n",
    "print(f\"Average loss sensitivity over test set: {sensitivity:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ART classifier for TabResNet\n",
    "art_classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    loss=criterion,\n",
    "    input_shape=X_test_np.shape[1],\n",
    "    nb_classes=2,\n",
    "    optimizer=optimizer,\n",
    "    clip_values=clip_values,\n",
    "    device_type=device\n",
    ")\n",
    "\n",
    "# Train the ART classifier with TabResNet\n",
    "# art_classifier.fit(X_train_np, y_train_np, batch_size=64, nb_epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuraacy Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 0.8474\n",
      "Accuracy on adversarial test examples: 0.8457\n",
      "Accuracy difference: 0.0017\n"
     ]
    }
   ],
   "source": [
    "predictions = art_classifier.predict(X_test_t)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == y_test_np) / len(y_test_np)\n",
    "print(f\"Accuracy on benign test examples: {accuracy:.4f}\")\n",
    "\n",
    "# Generate adversarial test examples\n",
    "attack = FastGradientMethod(estimator=art_classifier, eps=0.2)\n",
    "x_test_adv = attack.generate(x=X_test_np)\n",
    "\n",
    "# Evaluate the ART classifier on adversarial test examples\n",
    "\n",
    "predictions = art_classifier.predict(x_test_adv)\n",
    "accuracy_adv = np.sum(np.argmax(predictions, axis=1) == y_test_np) / len(y_test_np)\n",
    "print(f\"Accuracy on adversarial test examples: {accuracy_adv:.4f}\")\n",
    "\n",
    "print(f\"Accuracy difference: {accuracy - accuracy_adv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEVER-u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clever-u Score:\n",
    "\n",
    "score is a lower bound -> minimum perturbation size required to change the model's output to any wrong label\n",
    "\n",
    "“directional probes” (here 10*20=200) -> to get a reliable worst‐case slope estimate\n",
    "\n",
    "A higher score -> stronger local robustness\n",
    "\n",
    "range value = [0.0, radius]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in test set: 6513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing CLEVER-U: 100%|██████████| 6513/6513 [09:10<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVER-U score for TabResNet: 0.1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clever_scores = []\n",
    "num_samples = X_test_np.shape[0]\n",
    "print(f\"Number of samples in test set: {num_samples}\")\n",
    "# for sample in X_test_np[:num_samples]: \n",
    "for sample in tqdm(X_test_np[:num_samples], desc=\"Computing CLEVER-U\"):\n",
    "    # sample is shape (D,) → np.array([sample]) → (1, D)\n",
    "    c = clever_u(\n",
    "        classifier=art_classifier,\n",
    "        x=sample,\n",
    "        nb_batches=20,\n",
    "        batch_size=10,             # you only have one sample in the “batch”\n",
    "        norm=2,\n",
    "        radius=0.2,\n",
    "        verbose=False\n",
    "    )\n",
    "    clever_scores.append(c)\n",
    "\n",
    "clever = float(np.mean(clever_scores))\n",
    "print(f\"CLEVER-U score for TabResNet: {clever:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Sensitivity\n",
    "\n",
    "A **higher** sensitivity -> small input perturbations can cause **larger** changes in the loss -> indicating a \"steeper\" or potentially **less** robust local region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss sensitivity over test set: 0.000028\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "# Convert integer labels to one-hot:\n",
    "y_test_onehot = np.eye(num_classes)[y_test_np]\n",
    "\n",
    "sensitivity = loss_sensitivity(\n",
    "    classifier=art_classifier,\n",
    "    x=X_test_np,\n",
    "    y=y_test_onehot\n",
    ")\n",
    "\n",
    "print(f\"Average loss sensitivity over test set: {sensitivity:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ww-ihnSUcCz2",
    "8PJTcdufcCz8",
    "fu9sqMIgcCz-",
    "J22g3WEccC0I",
    "9TMwzwvNcC0K",
    "GfVfYd_ZcC0Q",
    "UgJ9XJAWDev3",
    "7N5q_LeLyo4x",
    "67Vzn6iuYS2q",
    "538PskKGALFU",
    "VJ0Mb-gM8lHj",
    "ki-IgGuwcC0k",
    "WeKnka-4W8Wr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fair-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
